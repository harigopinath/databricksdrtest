# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

pool:
  vmImage: ubuntu-latest

steps:

- script: |
    pip install databricks-cli
  displayName: 'install databricks cli'

- script: |
    databricks configure --token <<EOF
    https://adb-491463922244483.3.azuredatabricks.net
    dapi44a25ba7d3fe208c0168f6d4ea2bcb40
    EOF
  displayName: 'configure token for primary workspace'

- script: |
    wget https://github.com/databricks/terraform-provider-databricks/releases/download/v1.31.0/terraform-provider-databricks_1.31.0_linux_amd64.zip
    unzip terraform-provider-databricks_1.31.0_linux_amd64.zip
  displayName: 'Get the terraform exporter code'

- script: |
    ./terraform-provider-databricks_v1.31.0 exporter -listing=jobs,compute -directory output -skip-interactive
  displayName: 'run the terraform exporter'

- script: |
    databricks configure --token <<EOF
    https://adb-387883605664296.16.azuredatabricks.net/
    dapia5d70b9b3942e78a77200794b7074fda
    EOF
  displayName: 'configure token for secondary workspace'

- script: |
    cd output
    terraform init
    ./import.sh
    terraform apply <<EOF
    yes
    EOF
  displayName: 'import into secondary workspace'
